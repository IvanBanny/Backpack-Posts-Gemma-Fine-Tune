# Backpack-Posts-Gemma-Fine-Tune
My experiment in fine-tuning open-weights LLMs.
Tried fine-tuning Meta's Llama 3 for 8B and Google's Gemma for 7B and 2B.
Ended up using Gemma 2B, because it fit nicely into the VRAM of Google Colab.
